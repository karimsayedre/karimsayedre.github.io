<!DOCTYPE html>
<html lang="en">

<head>
    <title>Ray Tracing In One Weekend In CUDA</title>
    <meta charset="UTF-8">

    <!-- General Meta -->
    <meta name="description" content="A showcase of my projects and portfolio.">
    <link rel="icon" href="icons/Beyond.png">

    <!-- Open Graph Meta (for Facebook, LinkedIn, etc.) -->
    <meta property="og:title" content="Karim Sayed - Rendering Engineer">
    <meta property="og:description" content="A showcase of my projects and portfolio.">
    <meta property="og:image" content="https://karimsayedre.github.io/images/Pathtracing/0.jpg">
    <meta property="og:url" content="https://karimsayedre.github.io/">
    <meta property="og:type" content="website">

    <!-- Twitter Card Meta -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Karim Sayed - Rendering Engineer">
    <meta name="twitter:description" content="A showcase of my projects and portfolio.">
    <meta name="twitter:image" content="https://karimsayedre.github.io/images/Pathtracing/0.jpg">

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">

    <link rel="stylesheet" href="style/style.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
        crossorigin="anonymous"></script>

    <script src="scripts/images.js"></script>
    <script src="scripts/behaviour.js"></script>
    <script src="scripts/bars.js"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
        rel="stylesheet">

    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.css" />
    <!-- Highlight.js CSS theme -->
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/11.11.1/styles/line-numbers.min.css" />

    <!-- Highlight.js library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code&display=swap" rel="stylesheet">

</head>


<body>
    <div id="top"></div>

    <script>
        navbar();
    </script>

    <div class="container">

        <article>
            <div class="collapsible">
                <h1>CUDA Ray Tracing 3.6x Faster Than RTX: My CUDA Ray Tracing Journey</h1>
                <p><em>Note: This is a draft version. Final edits are still in progress. Feedback is welcome while final
                        edits are underway.</em></p>

            </div>

            <img class="photo" src="images/RTIOW/2560x1440_50depth_3000samples_3400ms.png">

            <section class="section-header">
                <h2>Introduction</h2>

                <p>
                    Alright, this headline is a bit <strong>click-baity</strong>, but it's actually true… kind of.
                    I'm comparing my CUDA path tracer against the
                    <a href="https://github.com/GPSnoopy/RayTracingInVulkan" target="_blank">RayTracingInVulkan</a>
                    repo by GPSnoopy:
                </p>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Renderer</th>
                            <th>Graphics API</th>
                            <th>Hardware Acceleration</th>
                            <th>Geometry Types</th>
                            <th>Performance (FPS)</th>
                            <th>GPU Time</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td class="spec-value">GPSnoopy's repository</td>
                            <td class="spec-value">Vulkan</td>
                            <td class="spec-value">RTX acceleration</td>
                            <td class="spec-value">Procedural sphere tracing + triangle modes</td>
                            <td class="spec-value fps-highlight">~33ms</td>
                            <td class="spec-value fps-highlight">~30 FPS</td>
                            <td class="spec-value">Cornell Box, Lucy, etc. - complex scenes</td>
                        </tr>
                        <tr>
                            <td class="spec-value">Mine</td>
                            <td class="spec-value">CUDA</td>
                            <td class="spec-value">No hardware RT cores</td>
                            <td class="spec-value">Procedural spheres only</td>
                            <td class="spec-value fps-highlight">~8ms</td>
                            <td class="spec-value fps-highlight">105 FPS</td>
                            <td class="spec-value">same resolution and settings</td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    Why is the Vulkan/RTX version slower? While this is probably not the only reason, one key issue is
                    that
                    procedural shaders are actually <em>frowned upon</em> for performance because RT cores perform best
                    with real
                    triangle geometry. Even with triangle support, all scenes in GPSnoopy's repo stay below 33
                    FPS—clearly
                    showing the limitations of a mixed procedural + hardware-centric pipeline.
                </p>
                <p>
                    Another reason might be the ray tracing pipeline itself. While powerful and flexible, the hardware
                    RT
                    pipeline often
                    incurs more overhead than inline ray tracing (Ray query). It tends to make heavy use of VRAM
                    bandwidth by moving
                    payload
                    data around between shader stages. On the other hand, inline ray tracing can keep most of the data
                    in registers, which is exactly what's happening in my implementation. So you can consider my
                    approach as <strong>inline ray tracing</strong>
                    This register-centric design drastically cuts down memory traffic and boosts performance.
                </p>

                <p>
                    So yes, it may sound like clickbait—but it's <em>technically</em> accurate, and when you dig into
                    sample rates, shader complexity, geometry types, and hardware, the numbers hold up. In this article,
                    I'll peel back the layers of how I squeezed 3.6x performance out through CUDA-level optimizations,
                    giving you an exciting taste of what's possible when you really dig deep into cache behavior,
                    register pressure, and GPU optimization.
                </p>

                <p>
                    As a graphics programmer, I’m constantly pushing the limits of what the GPU can do. But I realized
                    that knowing just high-level shading languages or APIs like Vulkan or DirectX wasn't enough—I needed
                    to understand the machine itself. CUDA gave me the lowest-level, most explicit way to explore how
                    GPUs schedule threads, manage memory, and hit (or miss) performance targets. And with the help of
                    <strong>Nsight Compute</strong>, I wasn’t just reading theory—I was hands-on, exploring real
                    bottlenecks, discovering how latency hiding works, learning about warp scheduling, cache behavior,
                    and so much more. It introduced me to performance concepts I hadn't encountered before, and grounded
                    them in actual numbers and experimentation.
                </p>

                <p>And I didn't want to "just learn a language." I wanted to <strong>learn CUDA as a suite of
                        tools</strong>, to
                    really get under the hood of how GPU code runs, stalls, and gets optimized. So I asked myself:
                    what's the best way to do that for a graphics programmer?
                </p>

                <p><strong>Answer:</strong> write a ray tracer from scratch in CUDA… and then squeeze it until it
                    screams.</p>

                <p>This article walks you through how I implemented a naive CUDA port of <em>Ray Tracing in One
                        Weekend</em>
                    that
                    ran at <strong>2.5 seconds per frame</strong>, and optimized it down to <strong>9
                        milliseconds</strong>. Along the way, I hit every wall I could—scoreboard stalls, branching
                    hell,
                    memory layout issues—and learned how to knock each one down.</p>

                <p>This isn't a language learning blog. It's an <strong>optimization story</strong>. A journey into how
                    GPUs
                    really work, and what it takes to make them fly.</p>

                <p>And if you're into ray tracing, performance hacking, or just enjoy watching frame times drop—you're
                    in
                    the right place.</p>

                <p> You can check out the source code along with it's commit history <a
                        href="https://github.com/karimsayedre/CUDA-Ray-Tracing-In-One-Weekend" target="_blank">HERE</a>.
                </p>

                <h3>Specifications:</h3>
                <p>
                    To give proper context to the performance numbers and optimizations discussed in this article, it's
                    important to understand the hardware I tested on. These specs shaped not only what was possible, but
                    also where the real bottlenecks and wins emerged during tuning.
                </p>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Component</th>
                            <th>Specification</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>CPU</td>
                            <td class="spec-value">i5 13600KF</td>
                        </tr>
                        <tr>
                            <td>GPU</td>
                            <td class="spec-value">RTX 3080 10GB Desktop</td>
                        </tr>
                        <tr>
                            <td>CUDA Version</td>
                            <td class="spec-value">12.9</td>
                        </tr>
                        <tr>
                            <td>Resolution</td>
                            <td class="spec-value">720x1280</td>
                        </tr>
                        <tr>
                            <td>Samples</td>
                            <td class="spec-value">30</td>
                        </tr>
                        <tr>
                            <td>Max Ray Depth</td>
                            <td class="spec-value">50</td>
                        </tr>
                    </tbody>
                </table>


                <h3>The Starting Point: A Naive CUDA Ray Tracer</h3>

                <p>Before any optimizations, I started with a direct CUDA port of <em>Ray Tracing in One Weekend</em>.
                    No
                    fancy tricks — just threads launching per pixel, tracing rays recursively <strong> plus traversing a
                        BVH, so yes, we're already not even as slow as big O of N here.</strong></p>

                <p>And it worked. Technically. But it was slow — <strong>2.5 seconds per frame</strong> kind of slow,
                    slower
                    than my old CPU version which was 1.5 seconds. Each
                    thread handled one pixel, there was no memory layout optimization, and no thought given
                    to how branching or recursion would behave on the GPU.</p>

                <p>This was intentional. I wanted to <strong>start <i>almost</i> from zero</strong>, actually, from
                    where I
                    thought was fast last time I tried to optimize it :)</p>

                <p>So with a chunky frame time and profiler in hand, I started breaking it down. Where was the time
                    going?
                    What was stalling? Why did a GPU that could chew through teraflops look like it was running on a
                    potato?
                </p>

                <p>Time to find out ... But first...</p>

                <h3>What CUDA Gives You (and What It Punishes You For)</h3>

                <p>CUDA is amazing because it gives you <strong>bare-metal control</strong> over how your code runs on
                    the
                    GPU. You're not writing shader code inside an engine or hoping a compiler figures things out —
                    you're
                    the compiler. You're the scheduler. You're the reason your app runs fast... or doesn't.</p>

                <p>But with that power comes the traps. And the first trap I stepped into was
                    <strong>recursion</strong>.
                </p>

                <p>Recursion on the GPU sounds elegant — until you realize it's <strong>kryptonite for
                        performance</strong>.
                    Why?</p>
                <ul>
                    <li><strong>Register pressure:</strong> every level of recursion eats more registers, and once
                        you're
                        out, you're spilling to memory.</li>
                    <li><strong>Local memory access:</strong> spilled data goes to local memory, which is slow, and you
                        don't get to control the layout.</li>
                    <li><strong>Stack usage:</strong> recursive calls build a big stack, and that stack sits in memory,
                        not
                        registers.</li>
                    <li><strong>Warp divergence:</strong> recursion usually means branching, and branching destroys SIMT
                        efficiency.</li>
                </ul>

                <p>Next mistake? I thought about trying inheritance for materials and objects. Turns out <strong>virtual
                        calls and dynamic polymorphism</strong> are not CUDA's friends. Even if it compiles, the cost is
                    brutal. You could go for <strong>static polymorphism</strong> (templates or CRTP), but that starts
                    to
                    bloat code size fast — and I honestly didn't push it far enough to know if the tradeoff was worth
                    it.
                </p>

                <p>On a brighter note, if you're coming from C++ graphics work, you'll be happy to know that <strong>GLM
                        works with CUDA</strong>. I used it throughout the project, and the performance hit was
                    negligible —
                    way better than writing custom vector/matrix types from scratch.</p>

                <p>Bottom line: CUDA gives you tools to go fast, but it doesn't forgive bad habits from CPU land. You
                    have
                    to think like the GPU... SIMT, parallel, latency hiding — or suffer.</p>


                <h3>Register Pressure: The Silent Killer of GPU Performance</h3>

                <p>One of the first things I had to come to terms with in CUDA is that <strong>registers are
                        everything</strong>. They're the fastest memory the GPU has, and CUDA tries to keep as much data
                    in
                    them as possible. But once you run out, you're in trouble.</p>

                <p><strong>Register pressure</strong> happens when your kernel uses too many registers per thread.
                    Sounds
                    innocent, but it can kill performance in more than one way:</p>

                <ul>
                    <li><strong>Lower occupancy:</strong> Each Streaming Multiprocessor (SM) has a limited number of
                        registers. If your kernel uses too many per thread, fewer threads can run at once, lowering
                        occupancy and throughput.</li>
                    <li><strong>Spilling to local memory:</strong> When the compiler can't fit everything in registers,
                        it
                        spills to local memory — which lives in global memory space. That's a huge latency hit.</li>
                    <li><strong>Instruction stalls:</strong> Excessive register usage can increase instruction
                        dependencies
                        and limit ILP (instruction-level parallelism), causing more stalls even within a warp.</li>
                </ul>

                <p>So, how do you know if register pressure is too high?</p>

                <ul>
                    <li><strong>Profiler tells you:</strong> Nsight Compute and Nsight Systems will show register count,
                        occupancy, and spill stores/loads. If you're seeing spill activity, you're over budget.</li>
                    <li><strong>Occupancy below expected levels:</strong> If you're running a small kernel but seeing
                        25-50%
                        occupancy, it's a red flag. Check the register usage per thread.</li>
                    <li><strong>Nsight Compute: </strong> it actually tells you! </li>
                </ul>
                <!-- Pro Tip Item - Alternative style -->
                <div class="gotcha-card pro-tip">
                    <div class="gotcha-marker pro-tip-marker"></div>
                    <div class="gotcha-content">
                        <h3>Pro Tips</h3>
                        <h4>Always compile with <code>-Xptxas=-v</code></h4>
                        <p>This will show information about each compiled function-how many register? how many bytes
                            spilled
                            to memory, how big is the stack frame?
                        </p>
                        <h4>Use Nsight Compute's built-in occupancy calculator!</h4>
                        <p>This is <strong> really</strong> useful, you give information about your kernel, it tells you
                            what's actually limiting your occupancy, neat!</p>
                    </div>
                </div>

                <p>In my case, recursion was the big offender — each level of recursion held ray state, intersection
                    info,
                    and more. Once I removed recursion and moved to an explicit stack in registers, I gained control. I
                    could reuse memory, limit stack depth, and avoid unnecessary spills.</p>

                <p>If you want your GPU code to fly, managing register pressure is a must. You're always balancing
                    performance against code clarity and flexibility — and in CUDA, it's better to stay lean.</p>
            </section>

            <section class="section-header">

                <h2 class="optimization-title">Optimization #1 — Aggressive Inlining via Header-Only CUDA Design</h2>

                <p>
                    In CUDA, performance often hinges on inlining. Unlike traditional C++, CUDA's
                    <code>__device__</code>
                    and <code>__host__ __device__</code> functions need to be visible at compile time for the compiler
                    to
                    inline them. Initially, I followed a standard C++ pattern: defining classes in <code>.cuh</code>
                    headers
                    and implementing them in separate <code>.cu</code> files.
                </p>

                <p>
                    That design turned out to be <strong>devastating for performance</strong>. NVCC wasn't able to
                    inline
                    key device functions, resulting in excessive register spilling, increased launch overhead, and
                    significant slowdown — even in release builds.
                </p>

                <p>
                    After switching to a <strong>header-only design</strong> (all device code inlined in
                    <code>.cuh</code>,
                    <i>well</i>, <code>.h</code> headers), everything changed: NVCC inlined everything into the
                    rendering
                    mega-kernel in release mode,
                    minimizing register usage and boosting performance.
                </p>

                <h3>Why CUDA Header-Only Design Matters</h3>
                <ol>
                    <li>
                        <strong>Limited Device Function Linkage:</strong> Device functions need to be visible at compile
                        time to be inlined. CUDA doesn't support separate compilation and linking as robustly as C++ for
                        device code.
                    </li>
                    <li>
                        <strong>Relocatable Device Code (RDC):</strong> You can enable it using <code>-rdc=true</code>,
                        but:
                        <ul>
                            <li>Compiles much slower.</li>
                            <li>Introduces link-time complexity.</li>
                            <li>May reduce inlining and hurt performance.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Inlining = Performance:</strong> For GPU kernels — especially mega-kernels in a path
                        tracer
                        — aggressive inlining means:
                        <ul>
                            <li>Fewer spills.</li>
                            <li>Less register pressure.</li>
                            <li>Better instruction scheduling.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Before vs After</h3>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Design</th>
                            <th>Inlining</th>
                            <th>Register Spills</th>
                            <th>Compile Time</th>
                            <th>Runtime Performance</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>.cu</code> per class</td>
                            <td>Poor</td>
                            <td>High</td>
                            <td>Fast</td>
                            <td class="bad">Slow</td>
                        </tr>
                        <tr>
                            <td><code>.cuh</code> header-only</td>
                            <td>Excellent</td>
                            <td>Minimal</td>
                            <td>Longer</td>
                            <td class="good">Fast</td>
                        </tr>
                    </tbody>
                </table>

                <p class="perf-note">
                    ✱ Verdict: <em>Go header-only for all device code unless you absolutely need RDC. Let the compiler
                        see
                        everything. Let it inline everything.</em>
                </p>

            </section>

            <section class="section-header">
                <h2>Optimization #2 — Killing Recursion with an Explicit Stack</h2>

                <p>To eliminate recursion and cut down register pressure, I rewrote the BVH traversal to use an
                    <strong>explicit stack in registers</strong>. The old code relied on a clean recursive structure
                    like
                    this:
                </p>

                <pre><code class="language-cpp">bool BVHNode::Hit(const Ray& r, float tMin, float tMax, HitRecord& rec) const
{
    if (!m_Box.Hit(r, tMin, tMax))
        return false;

    bool hitLeft  = m_Left->Hit(r, tMin, tMax, rec);
    bool hitRight = m_Right->Hit(r, tMin, hitLeft ? rec.T : tMax, rec);

    return hitLeft || hitRight;
}
</code></pre>

                <p>Readable? Yes. GPU-friendly? Not at all. Every call stacks up ray data, bounding boxes, hit records —
                    and
                    on a GPU, that means <strong>registers and stack memory</strong> fill up fast.</p>

                <p>The new version looks like this:</p>

                <pre><code class="language-cpp">__device__ bool Hit(const Ray& r, const Float tMin, Float tMax, HitRecord& rec) const
{
    Hittable* stack[16];
    int		  stack_ptr      = 0;
    bool	  hit_anything	 = false;
    Float	  closest_so_far = tMax;

    // Push root children (right first, then left to process left first)
    stack[stack_ptr++] = m_Right;
    stack[stack_ptr++] = m_Left;

    while (stack_ptr > 0)
    {
        Hittable* node = stack[--stack_ptr];

        // Early out: Skip nodes whose AABB doesn't intersect [tMin, closest_so_far]
        AABB box;
        node->GetBoundingBox(0, 0, box);
        if (!box.Hit(r, tMin, closest_so_far))
            continue;

        if (node->IsLeaf())
        {
            HitRecord temp_rec;
            if (node->Hit(r, tMin, closest_so_far, temp_rec))
            {
                hit_anything   = true;
                closest_so_far = temp_rec.T;
                rec			   = temp_rec;
            }
        }
        else
        {
            BVHNode* bvh_node = static_cast&lt;BVHNode*&gt;(node);
            // Push children in reverse order (right first, left next)
            stack[stack_ptr++] = bvh_node->m_Right;
            stack[stack_ptr++] = bvh_node->m_Left;
        }
    }
    return hit_anything;
}
</code></pre>

                <h3>Frame Time Performance</h3>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Frame Time</td>
                            <td>2.5s</td>
                            <td>300ms</td>
                            <td class="improvement">-2.2s (-88%)</td>
                        </tr>
                        <tr>
                            <td>Stack Memory/Thread</td>
                            <td>More</td>
                            <td>significantly less</td>
                            <td class="improvement">No need to dynamically set the stack size since it's much less and
                                is more predictable by the compiler.</td>
                        </tr>
                    </tbody>
                </table>




                <p>Now the traversal is entirely iterative, using a compact array on the stack (16 elements max
                    depending on
                    how many nodes there are) and
                    minimizing memory overhead. </p>

                <p>The key improvements:</p>

                <ul>
                    <li><strong>No recursion:</strong> No stack growth, no call overhead, no nested register use.</li>
                    <li><strong>Warp-coherent traversal:</strong> Front-to-back traversal increases chances of early
                        exit,
                        which avoids extra intersection tests.</li>
                </ul>

                <p>This one change gave me a big win in performance and stability — no more surprise stack overflows or
                    slowdowns due to spills.</p>
            </section>

            <section class="section-header">
                <h2 class="optimization-title">Optimization #3 — Don't Recompute What You Already Know</h2>
                <p>
                    Here's a simple but powerful axiom in real-time ray tracing:
                    <strong>Precompute what doesn't change.</strong> If you know you're going to need a value frequently
                    — especially one that's expensive to compute — then compute it once, store it, and reuse it.
                </p>

                <p>
                    Take the bounding box of a scene or a node in the BVH. If it's built once during scene setup and
                    never changes, there's no reason to recompute it every time a ray passes through. That's just
                    wasting cycles.
                </p>

                <p>
                    For example, this code:
                </p>

                <pre><code class="language-cpp">__device__ AABB HittableList::GetBoundingBox() const
{
    AABB outputBox;
    AABB tempBox;
    bool firstBox = true;

    for (uint32_t i = 0; i < m_Count; i++)
    {
        tempBox   = m_Objects[i]->GetBoundingBox(time0, time1);
        outputBox = firstBox ? tempBox : SurroundingBox(outputBox, tempBox);
        firstBox  = false;
    }

    return outputBox;
}</code></pre>

                <p>
                    ...does the job, but it's doing way too much. We already know what the result is going to be — it's
                    the
                    same every time. So instead, cache it in the BVH construction stage.
                </p>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Frame Time</td>
                            <td>300ms</td>
                            <td>200ms</td>
                            <td class="improvement">-100ms (-33.3%)</td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    Cleaner, faster, and more GPU-friendly.
                </p>

                <p>
                    Little changes like this can mean a lot when you're tracing millions of rays per frame. Always ask
                    yourself: "Can I compute this once and store it?" If yes — do it.
                </p>
            </section>

            <section class="section-header">
                <h2>Optimization #4 — Early Termination for Low Contributing Rays</h2>
                <p>
                    This one's simple but powerful. If a ray's contribution becomes negligible, we just stop tracing
                    it.
                    There's no point in wasting GPU cycles on a ray that's not adding anything visible to the final
                    image.
                </p>
                <pre><code class="language-cpp">// Early termination for very low contribution
if (fmaxf(cur_attenuation.x, fmaxf(cur_attenuation.y, cur_attenuation.z)) &lt; 0.001f)
    break;
</code></pre>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Frame Time</td>
                            <td>200ms</td>
                            <td>160ms</td>
                            <td class="improvement">-40ms (-20%)</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section class="section-header">
                <h2>Optimization #5 — Russian Roulette</h2>
                <p>
                    Early termination is good — but we can go further with <strong>Russian Roulette</strong>. After a
                    few bounces, we probabilistically decide whether a ray should continue or not, based on its current
                    energy.
                    This avoids wasting time on rays that contribute very little, while still preserving the statistical
                    integrity of the image.
                </p>
                <pre><code class="language-cpp">
// Russian Roulette
float surviveProbablity = fmaxf(cur_attenuation.x, fmaxf(cur_attenuation.y, cur_attenuation.z));
if (i > 3) {
    if (curand_uniform(&state) > surviveProbablity)
        break;
    cur_attenuation /= surviveProbablity;
}
</code></pre>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Frame Time</td>
                            <td>160ms</td>
                            <td>140ms</td>
                            <td class="improvement">-20ms (-12.5%)</td>
                        </tr>
                    </tbody>
                </table>

                <div class="perf-note">
                    Marginal here due to other bottlenecks, but still worth it.
                </div>

                <p>
                    For an extra boost, you can drop the <code>if (i &gt; 3)</code> condition and apply Russian Roulette
                    unconditionally. That took us down to <strong>120ms</strong> — though you might see more noise in
                    the image as a result.
                </p>
            </section>


            <section class="section-header">
                <h2>Optimization #6 — Structure of Arrays (SoA)</h2>
                <p>
                    Our original implementation leaned on inheritance and virtual dispatch, with every
                    object—spheres, BVH nodes,
                    and more—deriving from a common <code>Hittable</code> base. While it was easy to extend, GPUs
                    hate pointer
                    chasing and scattered memory.
                </p>
                <p>
                    Virtual function calls mean vtable indirection every ray hit, divergent control flow, and
                    fragmented data
                    layouts. Worse, accessing a <code>Sphere</code> pulled in multiple fields (center,
                    radius, materialIndex) from random heap locations—leading to uncoalesced accesses and cache
                    misses,
                    consuming precious memory bandwidth.
                </p>

                <pre><code class="language-cpp">class Hittable {
    virtual bool Hit(const Ray& ray, float tMin, float tMax, HitRecord& rec) const = 0;
};

class Sphere : public Hittable {
    Vec4 CenterAndRadius;
    uint32_t MaterialIndex;
    ...
};
class BVHNode : public Hittable { ... };
</code></pre>



                <p>
                    The cure was a full <strong>Structure of Arrays (SoA)</strong> rewrite. Flatten all properties
                    into flat arrays:
                </p>
                <pre><code class="language-cpp">// SoA data layout
struct Spheres {
    Vec4*     CenterAndRadius;         // packed sequentially
    uint32_t  count;
};

struct BVHSoA {
    uint32_t* Left;    // left-child or primitive index
    uint32_t* Right;   // right-child index
    AABB*     Bounds;  // bounding boxes
    uint32_t  Count;
    uint32_t  Root;
};</code></pre>
                <p>
                    No more vtables, no more scattered reads. Access patterns became predictable and
                    <strong>coalesced</strong>,
                    drastically reducing cache misses and bandwidth waste.
                </p>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>L1 Cache hit rates</td>
                            <td>81%</td>
                            <td>99%</td>
                            <td class="improvement">+18%</td>
                        </tr>

                        <tr>
                            <td>L2 Cache hit rates</td>
                            <td>82%</td>
                            <td>100%</td>
                            <td class="improvement">+17%</td>
                        </tr>

                        <tr>
                            <td>Frame Time</td>
                            <td>140ms</td>
                            <td>65ms</td>
                            <td class="improvement">-75ms (-53.6%)</td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    You might wonder: <em>how do we distinguish leaves (spheres) from internal BVH nodes?</em> Leaf
                    nodes always have right node that is <code> == UINT32_MAX</code>:
                </p>
                <pre><code class="language-cpp">const BVHSoA::BVHNode& node = params->BVH->m_Nodes[currentNode];

// Process leaf node
if (node.Right == UINT32_MAX)
{
    // Process sphere at index bvh.m_left[currentIndex]
} else {
    // Traverse children at bvh.m_left[currentIndex] and bvh.m_right[currentIndex]
}
</code></pre>
                <p>
                    Yes, there's a branch here—but it's predictable, and dwarfed by the savings from coalesced memory
                    access.
                    If you need to eliminate branches entirely, consider <strong>wavefront path tracing</strong>, where
                    you
                    process rays in large cohorts by bounce depth, grouping similar workloads to avoid divergent code
                    paths.
                    Or explore <strong>ray packet traversal</strong>, <strong>persistent threads</strong>, and other
                    advanced GPU
                    techniques for further gains.
                </p>


            </section>


            <section class="section-header">
                <h2>Optimization #7 — Four Levels of Ray-Slab Intersection Refinement</h2>
                <p>We improved BVH slab testing in four progressive steps, each trading complexity for fewer operations
                    inside the hot loop:</p>
                <ol>
                    <li>
                        <strong>Naïve Branch Swap:</strong>
                        <pre><code class="language-cpp">// Inside loop:
float tmin = (min_bound.x - ray.Origin().x) / ray.Direction().x;
float tmax = (max_bound.x - ray.Origin().x) / ray.Direction().x;
if (ray.Direction().x &lt; 0) std::swap(tmin, tmax);
// repeat for y and z, then combine</code></pre>
                        <p>This is simple and correct, but does two divides per axis and a branch per axis.</p>
                    </li>
                    <li>
                        <strong>Sign Precompute + Conditional Select:</strong>
                        <pre><code class="language-cpp">// Precompute signs once:
int signX = ray.Direction().x &lt; 0.0f;
int signY = ray.Direction().y &lt; 0.0f;
int signZ = ray.Direction().z &lt; 0.0f;

// Inside loop:
float tmin = ((signX ? max_bound.x : min_bound.x) - ray.Origin().x) / ray.Direction().x;
float tmax = ((signX ? min_bound.x : max_bound.x) - ray.Origin().x) / ray.Direction().x;
// repeat for y and z, then combine</code></pre>
                        <p>Removes dynamic branches inside the loop by precomputing the sign of each direction component
                            and using FSEL (or ?:) to select bounds based on sign, avoiding branches, but still two
                            divides per axis and select overhead.
                        </p>
                    </li>
                    <li>
                        <strong>Inverse Direction Hoisting:</strong>
                        <pre><code class="language-cpp">// Precompute once:
int signX = ray.Direction().x &lt; 0.0f;
int signY = ray.Direction().y &lt; 0.0f;
int signZ = ray.Direction().z &lt; 0.0f;
Vec3 invDir = 1.0f / ray.Direction();

// Inside loop:
float tmin = ((signX ? max_bound.x : min_bound.x) - ray.Origin().x) * invDir.x;
float tmax = ((signX ? min_bound.x : max_bound.x) - ray.Origin().x) * invDir.x;
// repeat for y and z</code></pre>
                        <p>Replaces divisions with multiplications by the precomputed inverse direction, eliminating
                            division
                            ops. BTW, compiler is smart enough it actually multplies by the reciprical of
                            <code class="language-cpp">ray.Direction()</code> instead.
                        </p>
                    </li>
                    <li>
                        <strong>FMA &amp; Origin-Offset Hoist:</strong>
                        <pre><code class="language-cpp">// Precompute once:
Vec3 invDir = 1.0f / ray.Direction();
Vec3 origMulInv = -ray.Origin() * invDir;

// Inside loop:
float tx0 = fmaf(invDir.x, min_bound.x, origMulInv.x);
float tx1 = fmaf(invDir.x, max_bound.x, origMulInv.x);
// and similarly for y, z</code></pre>
                        <p>Further consolidates subtraction and multiply into a single fused-multiply-add, removing all
                            subtractions and divisions inside the loop.</p>

                        <p>Each step progressively reduced operations per axis and branches, culminating in the
                            FMA-based
                            approach that is entirely arithmetic in registers.</p>
                        <div class="gotcha-card pro-tip">
                            <div class="gotcha-marker pro-tip-marker"></div>
                            <div class="gotcha-content">
                                <p>FMA performance here is a non-issue, I'm not just flexing—I'm showing off my CUDA
                                    prowess 😎.
                                    But hey, got to demonstrate I know my hardware! 🚀</p>

                            </div>
                        </div>
            </section>

            <section class="section-header">
                <h2>Optimization #8 — Surface Area Heuristic (SAH) BVH Construction</h2>
                <p>
                    Constructing a BVH by simply splitting primitives in half along an axis is easy—but not optimal. The
                    <strong>Surface Area Heuristic (SAH)</strong> chooses split planes based on minimizing the expected
                    cost of
                    ray traversal, taking into account both the surface areas of child nodes and the number of
                    primitives.
                </p>
                <p>Basic SAH pseudo-code:</p>
                <pre><code class="language-cpp">// For each axis (x, y, z):
for (int axis = 0; axis < 3; ++axis) {
  // Sort primitives by centroid along this axis
  sort(primitives.begin(), primitives.end(), compareCentroid(axis));

  // Evaluate split at each boundary
  for (int i = 1; i < N; ++i) {
    float leftArea  = computeBoundsArea(primitives[0..i-1]);
    float rightArea = computeBoundsArea(primitives[i..N-1]);
    float cost =  traversalCost + (leftArea/totalArea) * i * intersectionCost
                 + (rightArea/totalArea) * (N-i) * intersectionCost;
    if (cost < bestCost) {
      bestCost = cost;
      bestSplit = i;
    }
  }
}
// Recursively build left and right using bestSplit
</code></pre>
                <p>
                    Using SAH typically increases build time but yields much better trees, reducing traversal steps per
                    ray.
                </p>
                <p>
                    You can further optimize SAH builds by using binning (grouping primitives into fixed buckets) to
                    avoid
                    sorting at every split, bringing build times to near-linear complexity while preserving most of the
                    quality
                    benefits.
                </p>
            </section>

            <section class="section-header">
                <h2>Optimization #9 — Alignment and Cacheline Efficiency</h2>
                <p>
                    Closely related to our Structure of Arrays (SoA) optimization, I found that <strong>data
                        alignment</strong> plays a massive role in
                    memory throughput on the GPU. While SoA improves access patterns and memory coalescing, improperly
                    aligned data can still bottleneck performance due to cacheline splits and inefficient memory
                    instructions.
                </p>
                <p>
                    Take <code>AABB</code> for example: it's composed of two <code>Vec3</code> members (<code>min</code>
                    and <code>max</code> bounds). In our case, <code>Vec3</code> was defined as:
                </p>
                <pre><code class="language-cpp">using Vec3 = glm::vec&lt;3, float, glm::aligned_mediump&gt;;</code></pre>
                <p>
                    While <code>glm::aligned_mediump</code> suggests alignment, it <strong>does not</strong> enforce
                    16-byte alignment in CUDA. That's because GLM's
                    CUDA support is currently <strong>experimental</strong>, and its alignment attributes do not
                    translate reliably to device code.
                    This explains why <code>glm::aligned_vec3</code> or any of GLM's alignment variants may not behave
                    as expected when compiled with NVCC. Also, yes I tried <code>glm::vec4</code> and I noticed no
                    differences, it may have even increased register pressure, then I tried this which basically made no
                    difference in my case:
                <pre><code class="language-cpp">struct alignas(16) Vec3
{
    glm::vec<3, float, glm::aligned_mediump> V;
    float                                    Padding;
};</code></pre>
                </p>
                <p>
                    I experimented with writing my own math types to enforce alignment, and even tested alternatives
                    like using
                    <code>__half</code> to reduce memory bandwidth. However, these experiments yielded no measurable
                    performance gain.
                    On older NVIDIA architectures (e.g. Turing), half-precision could offer up to 2x the FLOPs compared
                    to full-precision,
                    but on <strong>Ampere and beyond</strong> (which is what I'm running on), FP16 and FP32 CUDA core
                    throughput
                    is equivalent. So, using <code>__half</code> didn't help—and added complexity instead.
                </p>

                <p>
                    That one alignment mismatch, however, had a serious performance impact. CUDA ended up emitting
                    <strong>twice as many</strong> global
                    memory load instructions (<code>LDG</code>) for accessing misaligned <code>AABB</code> data, which
                    translated into significantly higher
                    memory pressure:
                </p>
                <p>
                    Despite the name, <code>aligned_mediump</code> does <strong>not</strong> guarantee 16-byte alignment
                    on CUDA. A <code>Vec3</code> still only occupies 12 bytes,
                    and unless you explicitly pad or align your structs, an <code>AABB</code> ends up 24 bytes
                    wide—leading to misaligned memory accesses and cacheline overlaps
                    when read in bulk.
                </p>
                <p>
                    This had a massive impact on performance. Before aligning our hot data structures (like BVH bounds),
                    CUDA would emit
                    <strong>twice as many</strong> <code>LDG</code> (load global) instructions (4 compared to 2 per
                    AABB). Global memory accesses
                    exploded:
                </p>

                <h3>Global Memory Performance</h3>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Before</th>
                            <th>After</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Global Memory Requests</td>
                            <td>~490M requests</td>
                            <td>~260M requests</td>
                            <td class="improvement">-230M (-47%)</td>
                        </tr>
                        <tr>
                            <td>Frame Time</td>
                            <td>10ms</td>
                            <td>8ms</td>
                            <td class="improvement">-2ms (-20%)</td>
                        </tr>
                    </tbody>
                </table>


                <p>
                    The fix? Use manual padding or explicit alignment declarations:
                </p>
                <pre><code class="language-cpp">using Vec3 = glm::vec&lt;3, float, glm::aligned_mediump&gt;;

struct alignas(32) AABB
{
    __align__(16) Vec3 Min;
    __align__(16) Vec3 Max;
};</code></pre>

                <p>
                    Alternatively, switch to <code>glm::vec4</code> if the extra float isn't a concern. The key is
                    making sure each element of your
                    SoA data layout is 16-byte aligned, especially if you access it frequently inside inner loops.
                </p>
                <p>
                    CUDA's memory subsystem heavily favors aligned and coalesced reads. Without alignment, even perfect
                    SoA layouts can suffer from
                    doubled memory traffic. With alignment, you get fewer loads and fewer cacheline splits.
                </p>
            </section>

            <section class="section-header" id="optimization-10">
                <h2>Optimization #10 — Memory Alignment for Efficient Global Loads</h2>
                <p>
                    This optimization stems from the SoA (Structure of Arrays) layout, but digs deeper into memory
                    alignment and
                    the inefficiencies of loading 12-byte values (like <code>glm::vec3</code>) from global memory on the
                    GPU.
                </p>

                <p>
                    Consider an <code>AABB</code> represented by two <code>Vec3</code>s. Even when using
                    <code>using Vec3 = glm::vec&lt;3, float, glm::aligned_mediump&gt;;</code>, CUDA does not align the
                    struct to 16 bytes.
                    As a result, memory accesses to these unaligned <code>vec3</code>s lead to inefficient load
                    instructions at the PTX level.
                </p>

                <p>
                    In PTX, there's no such thing as a 12-byte <code>LDG.E.96</code>. Instead, the compiler
                    emits:
                </p>

                <pre><code>ld.global.v2.f32 %r1, [%addr];      // loads x and y
ld.global.f32    %r2, [%addr+8];    // loads z</code></pre>

                <p>
                    This means:
                <ul>
                    <li>Two separate memory instructions per <code>vec3</code></li>
                    <li>Increased register usage</li>
                    <li>Higher chance of cacheline splits (especially across 128B boundaries)</li>
                    <li>Uncoalesced memory access patterns across warps</li>
                </ul>
                </p>

                <p>
                    On the other hand, if the data is aligned to 16 bytes (e.g., using <code>vec4</code> or manual
                    padding),
                    the compiler emits a single <code>ld.global.v4.f32</code> instruction:
                </p>

                <pre><code>ld.global.v4.f32 %r1, [%addr];</code></pre>

                <p>
                    This load is:
                <ul>
                    <li>Coalesced across threads</li>
                    <li>Aligned with cacheline boundaries</li>
                    <li>Issued as a single instruction</li>
                    <li>Less likely to cause redundant memory traffic</li>
                </ul>
                </p>

                <p>
                    To fix the alignment, I modified my layout so that the <code>AABB</code> used properly aligned
                    members:
                </p>

                <pre><code>struct alignas(16) AlignedVec3 {
    float x, y, z, pad;
};

struct AABB {
    AlignedVec3 min;
    AlignedVec3 max;
};</code></pre>

                <p>
                    This change alone reduced frame time from <strong>10ms to 8ms</strong> and cut global memory
                    requests from
                    <code>~490 million</code> to <code>~260 million</code> — a <strong>~84.58%</strong> reduction —
                    simply because
                    the compiler emitted half the number of <code>LDG</code> instructions.
                </p>

                <p>
                    This issue also highlights a limitation in GLM's CUDA support. While I used
                    <code>glm::vec&lt;3, float, glm::aligned_mediump&gt;</code>, GLM does not currently enforce 16-byte
                    alignment in CUDA
                    kernels because its support for CUDA is still <strong>experimental</strong>.
                </p>

                <p>
                    I experimented with writing a minimal custom math library to address this, even testing
                    half-precision floats (<code>__half</code>),
                    but saw no performance improvement. On modern hardware like Ampere (which I'm running on),
                    <strong>half and full floats
                        have the same throughput</strong>. Only older architectures offered double the FLOPs for
                    <code>__half</code>.
                </p>

                <p>
                    In short, aligning critical structs like AABBs ensures that memory loads are coalesced and avoid
                    split cacheline accesses.
                    Even with an ideal SoA layout, unaligned <code>vec3</code>s force the compiler to emit multiple load
                    instructions,
                    effectively doubling the number of global memory ops.
                </p>
            </section>

            <section class="section-header" id="optimization-11">
                <h2>Optimization #11 — Using Constant Memory Instead of Global Memory</h2>

                <p>
                    One of the most effective register-saving optimizations in CUDA is proper use of the
                    <code>__constant__</code> memory space.
                    This is especially beneficial when dealing with per-frame parameters that:
                </p>
                <ul>
                    <li>Change infrequently (typically once per frame).</li>
                    <li>Are read-only from the kernel's perspective.</li>
                    <li>Are accessed uniformly across threads in a warp (broadcast access).</li>
                </ul>

                <p>
                    In my path tracer, I created a <code>RenderParams</code> structure that encapsulates all such
                    parameters:
                </p>

                <pre><code>struct RenderParams
{
    Hitables::PrimitiveList* __restrict__ List {};
    BVH::BVHSoA* __restrict__ BVH {};
    Mat::Materials* __restrict__ Materials {};
    uint32_t* __restrict__ RandSeeds {};
    CameraPOD Camera {};
    float4    ResolutionInfo {}; // x: x Pixel Size, y: y Pixel Size, z: width, w: height
    cudaSurfaceObject_t Image {};
    uint32_t m_SamplesPerPixel {};
    uint32_t m_MaxDepth {};
    float    m_ColorMul {};
};</code></pre>



                <p>
                    Constant memory on CUDA devices is a small (typically 64 KB) memory region optimized for broadcast
                    access. When every thread in a warp reads the same address from constant memory, the value is
                    broadcast efficiently to all threads. This is ideal for things like camera parameters, resolution
                    information, and control settings that are used consistently by every thread.
                </p>

                <p>
                    What makes this optimization powerful is that:
                <ul>
                    <li>The compiler doesn't allocate registers for constant values — they are referenced directly.</li>
                    <li>There's no need to pass many arguments through kernel parameters, avoiding register pressure
                        from parameter passing.</li>
                    <li>Uniform access patterns mean near-zero latency access from constant cache. Especially that they
                        are not accessed that frequently in hot code.</li>
                </ul>
                </p>

                <p>This is actually how the kernel signature looked like:
                <pre><code class="language-cpp">__global__ void InternalRender(cudaSurfaceObject_t fb, BVHSoA* __restrict__ world, HittableList* __restrict__ list, Materials* __restrict__ materials, uint32_t max_x, uint32_t max_y, Camera* camera, uint32_t samplersPerPixel, Float colorMul, uint32_t maxDepth, uint32_t* randSeeds);</code></pre>
                </p>

                <p>
                    My code passed such values as regular kernel arguments or global memory
                    pointers. That forced each thread to load and hold these values separately, increasing both register
                    pressure and global memory traffic. Now:

                <pre><code>struct RenderParams
{
    Hitables::PrimitiveList* __restrict__ List {};
    BVH::BVH* __restrict__ BVH {};
    Mat::Materials* __restrict__ Materials {};
    uint32_t* __restrict__ RandSeeds {};
    CameraPOD			Camera {};
    float4				ResolutionInfo {}; // x: x Pixel Size, y: y Pixel Size, z: width, w: height
    cudaSurfaceObject_t Image {};
    uint32_t			m_SamplesPerPixel {};
    uint32_t			m_MaxDepth {};
    float				m_ColorMul {};
};

__constant__ inline RenderParams d_Params;
__global__ void InternalRender();</code></pre>
                </p>




                <p>
                    After moving the relevant data to <code>__constant__</code> memory, I observed a noticeable drop in
                    register usage (especially in occupancy-limited kernels) and better caching behavior. Since the
                    compiler knows these values are read-only and shared across all threads, it can aggressively
                    optimize access.
                </p>


                <p>
                    Again, I carefully chose what to place in constant memory:
                </p>
                <ul>
                    <li>Only data that remains fixed during kernel execution (per-frame or static)</li>
                    <li>Data accessed frequently and uniformly across all threads</li>
                    <li>No large arrays or per-pixel/per-thread state</li>
                </ul>

                <p>
                    Examples of values that went into constant memory:
                </p>
                <ul>
                    <li><code>CameraPOD</code>: used every time a ray is generated</li>
                    <li><code>ResolutionInfo</code>: used in every pixel calculation</li>
                    <li><code>m_SamplesPerPixel</code>, <code>m_MaxDepth</code>, <code>m_ColorMul</code>: constant loop
                        bounds or scaling factors</li>
                </ul>

                <p>
                    Meanwhile, heavy or frequently written buffers (like materials or the output image) remain in global
                    memory, passed via pointers in the constant struct, so they're still accessible efficiently but not
                    directly stored in constant memory.
                </p>

                <p>
                    With this change, the compiler became less aggressive in spilling registers, occupancy improved, and
                    runtime performance became more consistent — especially in large scenes with high path depth.
                </p>
                <h3>Shared vs Constant vs Global Memory</h3>

                <p>
                    To clarify where constant memory fits in CUDA's memory hierarchy, here's a high-level comparison:
                </p>

                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Memory Type</th>
                            <th>Scope</th>
                            <th>Access Pattern</th>
                            <th>Speed</th>
                            <th>Capacity</th>
                            <th>Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Constant</strong></td>
                            <td>Global (but read-only)</td>
                            <td>Broadcast</td>
                            <td>Very Fast (cached)</td>
                            <td>64 KB total</td>
                            <td>Uniform read-only data (e.g. parameters, transforms)</td>
                        </tr>
                        <tr>
                            <td><strong>Shared</strong></td>
                            <td>Per-Block</td>
                            <td>Cooperative (explicit)</td>
                            <td>Very Fast (SRAM)</td>
                            <td>Up to 100 KB per block (depending on architecture)</td>
                            <td>Thread collaboration, caching, small local working sets</td>
                        </tr>
                        <tr>
                            <td><strong>Global</strong></td>
                            <td>Global</td>
                            <td>Scattered</td>
                            <td>Slow (DRAM)</td>
                            <td>Many GBs</td>
                            <td>Large datasets, outputs, scene data, textures</td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    In simple terms:
                <ul>
                    <li>Use <strong>constant memory</strong> for small, uniform, read-only data that is shared across
                        all threads.</li>
                    <li>Use <strong>shared memory</strong> for fast, temporary per-block data, ideally when threads
                        cooperate.</li>
                    <li>Use <strong>global memory</strong> for everything else — especially large and frequently updated
                        data.</li>
                </ul>
                </p>

                <p>
                    In the case of my path tracer, <code>RenderParams</code> fits constant memory perfectly — it is
                    fixed across the frame, read-only, and used uniformly by all threads. This decision directly
                    improved performance by reducing register usage and leveraging the constant cache's broadcast
                    mechanism.
                </p>
            </section>

            <section class="section-header" id="optimization-12">
                <h2>Optimization #12 — Prefer <code>&lt;cmath&gt;</code> Intrinsics Over <code>&lt;algorithm&gt;</code>
                    in CUDA</h2>

                <p>
                    When writing CUDA <code>__device__</code> code, avoid using standard C++ functions like
                    <code>std::max</code>,
                    <code>std::min</code>, or <code>std::fma</code>. While they work on the host, they tend to produce
                    bloated or
                    inefficient PTX, leading to more global loads, predicate-based conditionals, and elevated register
                    pressure. In
                    contrast, using the float-specialized intrinsics from <code>&lt;cmath&gt;</code>—like
                    <code>std::fmaxf</code>, <code>std::fminf</code>, and <code>std::fmaf</code>—results in streamlined
                    hardware
                    instructions with significantly better performance.
                </p>

                <h3>Case Study: Ray-AABB Intersection</h3>
                <p>
                    This optimization had direct impact in our BVH traversal code, where millions of ray-AABB tests are
                    done per
                    frame. Switching from the generic <code>std::fma</code> and <code>std::max</code> to the intrinsic
                    float versions
                    led to a frame time drop from <strong>12ms</strong> to <strong>9ms</strong>, and reduced instruction
                    count.
                </p>

                <pre><code class="language-cpp">// AABB intersection (optimized)
const AABB& bounds = params->BVH->m_Bounds[node.Left];
float tx0 = std::fmaf(invDir.x, bounds.Min.x, rayOriginMulNegInvDir.x);
float tx1 = std::fmaf(invDir.x, bounds.Max.x, rayOriginMulNegInvDir.x);
float ty0 = std::fmaf(invDir.y, bounds.Min.y, rayOriginMulNegInvDir.y);
float ty1 = std::fmaf(invDir.y, bounds.Max.y, rayOriginMulNegInvDir.y);
float tz0 = std::fmaf(invDir.z, bounds.Min.z, rayOriginMulNegInvDir.z);
float tz1 = std::fmaf(invDir.z, bounds.Max.z, rayOriginMulNegInvDir.z);

float tEnter = std::fmaxf(
                 std::fmaxf(std::fminf(tx0, tx1), std::fminf(ty0, ty1)),
                 std::fmaxf(std::fminf(tz0, tz1), tmin));
float tExit = std::fminf(
                std::fminf(std::fmaxf(tx0, tx1), std::fmaxf(ty0, ty1)),
                std::fminf(std::fmaxf(tz0, tz1), tmax));
bool hit = (tEnter <= tExit);</code></pre>

                <h3>Performance Breakdown</h3>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Function</th>
                            <th><code>std::fmaxf</code> / <code>std::fmaf</code></th>
                            <th><code>std::max</code> / <code>std::fma</code></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>CUDA Compatibility</td>
                            <td>✅ Built-in device intrinsics</td>
                            <td>⚠️ May fail to compile or introduce overhead</td>
                        </tr>
                        <tr>
                            <td>PTX Output</td>
                            <td>✅ Maps directly to FMNMX, FFMA, FMUL</td>
                            <td>❌ Emits FSETP, FSEL, conditional branches</td>
                        </tr>
                        <tr>
                            <td>Register Pressure</td>
                            <td>✅ Minimal</td>
                            <td>❌ Higher due to control flow</td>
                        </tr>
                        <tr>
                            <td>NaN Behavior</td>
                            <td>✅ IEEE 754 compliant</td>
                            <td>❌ Inconsistent or undefined</td>
                        </tr>
                        <tr>
                            <td>Performance (in hot path)</td>
                            <td><strong>9ms</strong> total frame time</td>
                            <td><strong>12ms</strong> total frame time</td>
                        </tr>
                    </tbody>
                </table>

                <h3>PTX-Level Insight</h3>
                <ul>
                    <li>
                        <code>std::fmaxf</code>, <code>std::fminf</code>, <code>std::fmaf</code> compile down to
                        single-instruction
                        <code>FMNMX</code>, <code>FFMA</code>, or <code>FMUL</code> operations.
                    </li>
                    <li>
                        <code>std::max</code> or <code>glm::max</code> (when not specialized) emit <code>FSETP</code>
                        predicate
                        logic followed by <code>FSEL</code> conditionals, increasing instruction count and divergence
                        risks.
                    </li>
                    <li>
                        The generic versions can also trigger redundant loads and spills—particularly harmful in
                        high-traffic,
                        tight loops.
                    </li>
                </ul>

                <h3>Best Practices</h3>
                <ul>
                    <li>Always prefer <code>std::fmaxf</code>, <code>std::fminf</code>, and <code>std::fmaf</code> when
                        working with <code>float</code> in CUDA.</li>
                    <li>Use <code>std::fmax</code>, <code>std::fmin</code>, and <code>std::fma</code> only for
                        <code>double</code>.
                    </li>
                    <li>Avoid <code>std::max</code> and <code>std::min</code> entirely in <code>__device__</code> code.
                    </li>
                </ul>

                <p>
                    Replacing generic <code>&lt;algorithm&gt;</code> utilities with <code>&lt;cmath&gt;</code>
                    intrinsics is
                    not a micro-optimization—it's a major win in performance-critical kernels. In our case, it shaved
                    off
                    <strong>3ms per frame</strong> and greatly simplified the PTX output.
                </p>
            </section>

            <section class="section-header" id="optimization-13">
                <h2>Optimization #13 — Roll Your Own RNG (LCG + Hash) Instead of <code>curand</code></h2>

                <p>
                    When working with real-time GPU workloads like path tracing, CUDA's
                    <code>curand</code> library
                    is often overkill. While <code>curand</code> provides high-quality random numbers, it incurs
                    substantial overhead,
                    including register pressure, memory footprint, and instruction complexity—none of which play well in
                    tight GPU loops.
                </p>

                <h3>Custom RNG: PCG Hash + LCG</h3>
                <p>
                    For performance-critical use cases where statistical quality can be relaxed, combining a simple
                    PCG-style hash with a
                    Linear Congruential Generator (LCG) offers a great balance. This approach:
                </p>

                <ul>
                    <li>Uses just a few integer ops</li>
                    <li>Compiles to clean, branchless PTX</li>
                    <li>Works in both <code>__host__</code> and <code>__device__</code> contexts</li>
                    <li>Requires no global state or memory buffers</li>
                </ul>

                <h4>Code</h4>
                <pre><code class="language-cpp">[[nodiscard]] __device__ __host__ __forceinline__ uint32_t PcgHash(const uint32_t input)
{
    const uint32_t state = input * 747796405u + 2891336453u;
    const uint32_t word  = ((state >> ((state >> 28u) + 4u)) ^ state) * 277803737u;
    return (word >> 22u) ^ word;
}

[[nodiscard]] __device__ __host__ __forceinline__ uint32_t RandomInt(uint32_t& seed)
{
    return (seed = (1664525u * seed + 1013904223u));
}

[[nodiscard]] __device__ __host__ __forceinline__ float RandomFloat(uint32_t& seed)
{
    // Fast float generation from masked bits
    return static_cast&lt;float&gt;(RandomInt(seed) & 0x00FFFFFF) / static_cast&lt;float&gt;(0x01000000);
}

[[nodiscard]] __device__ __host__ __forceinline__ Vec2 RandomVec2(uint32_t& seed)
{
    return { RandomFloat(seed), RandomFloat(seed) };
}

[[nodiscard]] __device__ __host__ __forceinline__ Vec3 RandomVec3(uint32_t& seed)
{
    return { RandomFloat(seed), RandomFloat(seed), RandomFloat(seed) };
}</code></pre>

                <h3>Performance Comparison</h3>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>curand</th>
                            <th>Custom RNG</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Register Usage</td>
                            <td>❌ High (12-16 registers per thread)</td>
                            <td>✅ Low (2-4 registers)</td>
                        </tr>
                        <tr>
                            <td>PTX Complexity</td>
                            <td>❌ Dozens of ops per sample</td>
                            <td>✅ ~4-5 ALU ops per generated float</td>
                        </tr>
                        <tr>
                            <td>Global Memory</td>
                            <td>❌ Requires setup + state buffers</td>
                            <td>✅ Stateless, local only</td>
                        </tr>
                        <tr>
                            <td>Speed</td>
                            <td>❌ Slower in hot paths</td>
                            <td>✅ Fast even in tight loops</td>
                        </tr>
                        <tr>
                            <td>Randomness Quality</td>
                            <td>✅ High (suitable for MC methods)</td>
                            <td>⚠️ Lower (good enough for visual noise)</td>
                        </tr>
                    </tbody>
                </table>

                <h3>When to Use</h3>
                <ul>
                    <li>✅ Real-time rendering (e.g., screen-space sampling, reservoir sampling, importance sampling)
                    </li>
                    <li>✅ Denoising debug masks, visualization noise</li>
                    <li>❌ Offline film rendering or statistical tests</li>
                </ul>

                <p>
                    In our renderer, replacing <code>curand_uniform</code> with this custom generator cut per-frame
                    execution time
                    by a noticeable margin. Combined with register pressure savings, it enabled tighter occupancy and
                    better warp execution
                    in sampling-heavy shaders.
                </p>

                <p>
                    If you're hitting performance bottlenecks in CUDA sampling loops and don't need cryptographic
                    randomness, trade
                    entropy for speed—it's worth it.
                </p>
            </section>


            <section class="section-header" id="optimization-13">
                <h2>Optimization #14 — Branchless Material Sampling &amp; Evaluation</h2>

                <p>
                    My old implementation uses a <code>switch</code> over material types (Lambert, Metal,
                    Dielectric), each with its own
                    branching logic and random sampling. While straightforward, this approach incurs warp divergence,
                    extra branches, and duplicated work in hot inner loops.
                </p>

                <h3>Original Branching Version</h3>
                <pre><code class="language-cpp">__device__ inline bool Material::Scatter(
    const Ray& ray, const HitRecord& rec, vec3 &attenuation, Ray &scattered,
    curandState* state) const
{
    switch (m_Type) {
      case MaterialType::Lambert: { /* diffuse logic */ }
      case MaterialType::Metal:   { /* metal logic */ }
      case MaterialType::Dielectric: { /* dielectric logic */ }
    }
    return false;
}</code></pre>

                <p>
                    Every ray hit executes multiple <code>if</code> or <code>switch</code> evaluations, and each path
                    has its own random
                    fetches and normalization calls—magnified millions of times per frame.
                </p>

                <h3>Branchless Unified Version</h3>
                <p>
                    We replace per-material branches with a single, weighted blend of Lambert, Metal, and Dielectric
                    contributions.
                    Random directions, Fresnel reflectance, and face-normal tests are all computed up front, then
                    linearly combined
                    based on pre-normalized weights.
                </p>
                <pre><code class="language-cpp">[[nodiscard]] __device__ __host__ CPU_ONLY_INLINE bool Scatter(Ray& ray, const HitRecord& rec, Vec3& attenuation, uint32_t& randSeed)
{
    const RenderParams* params = GetParams();

    const Vec4& albedoIor	  = params->Materials->AlbedoIOR[rec.PrimitiveIndex];
    const auto& [flags, fuzz] = params->Materials->FlagsFuzz[rec.PrimitiveIndex];

    // normalize weights
    const float sumW  = flags.x + flags.y + flags.z + 1e-6f;
    const Vec3	normW = flags / sumW;

    const Vec3 rand3 = RandomVec3(randSeed);

    // Precompute directions
    const Vec3& unitDir	   = ray.Direction; // already normalized
    const Vec3	lambertDir = glm::normalize(rec.Normal + rand3);
    const Vec3	metalRef   = reflect(unitDir, rec.Normal);
    const Vec3	metalDir   = glm::normalize(metalRef + fuzz * rand3);

    // Dielectric components using faceNormal
    const float frontFaceMask = dot(unitDir, rec.Normal) &lt; 0.0f;
    const Vec3	faceNormal	  = frontFaceMask * rec.Normal + (1.0f - frontFaceMask) * -rec.Normal;
    const float cosTheta	  = std::fminf(dot(-unitDir, faceNormal), 1.0f);
    const float sinTheta	  = std::sqrtf(std::fmaxf(0.0f, 1.0f - cosTheta * cosTheta));
    const float etaiOverEtat  = frontFaceMask * (1.0f / albedoIor.W) + (1.0f - frontFaceMask) * albedoIor.W;
    const float cannotRefract = etaiOverEtat * sinTheta &gt; 1.0f;
    const float reflectProb	  = cannotRefract + (1.0f - cannotRefract) * Schlick(cosTheta, albedoIor.W);
    const float isReflect	  = rand3.x &lt; reflectProb;

    const Vec3 refracted = RefractBranchless(unitDir, faceNormal, etaiOverEtat);
    const Vec3 dielecDir = isReflect * reflect(unitDir, faceNormal) + (1.0f - isReflect) * refracted;

    // Composite direction and normalize
    const Vec3 dir = lambertDir * normW.x + metalDir * normW.y + dielecDir * normW.z;
    ray			   = Ray(rec.Location, normalize(dir));

    // Branchless attenuation: lambert & metal albedo, dielectric = 1
    const Vec3 att = reinterpret_cast&lt;const Vec3&&gt;(albedoIor) * (normW.x + normW.y) + Vec3(1.0f) * normW.z;
    attenuation *= att * sumW;

    // Early exit on no scatter
    const float scatterDot = dot(dir, rec.Normal);
    return (scatterDot > 0.0f) || (flags.z > 0.0f);
}</code></pre>

                <h3>Why This Matters</h3>
                <ul>
                    <li><strong>Zero material branches:</strong> All threads follow the same arithmetic path since there
                        are no if/switch statements, so, reduced divergence.</li>
                    <li><strong>Single random pool:</strong> One RNG per thread, reused for all contributions.</li>
                    <li><strong>Blendable:</strong> Easily support mixtures of materials (e.g., layered or coated
                        surfaces).</li>
                </ul>

                <p>
                    In my benchmarks, this branchless approach improved warp efficiency and reduced overall shader time
                    compared to the classic <code>switch</code>-based sampler,
                    leading to smoother
                    frame rates in complex scenes.
                </p>
            </section>

            <section class="section-header" id="optimization-14">
                <h2>Optimization #15 — Bypass CPU Staging with CUDA↔OpenGL Interop</h2>

                <p>
                    Early on, I rendered each frame by copying the CUDA output into an <code>sf::Image</code> (CPU
                    memory), then letting SFML
                    upload that image to an OpenGL texture under the hood. That CPU ↔ GPU round-trip cost was small—but
                    measurable:
                </p>

                <pre><code class="language-cpp">// Pseudocode of the old pipeline
cudaMemcpy(hostBuffer, deviceBuffer, size, cudaMemcpyDeviceToHost);
sf::Image img;
img.create(width, height, hostBuffer);
sf::Texture tex;
tex.update(img);  // uploads back to GPU (OpenGL context)</code></pre>

                <p>
                    At 1280x720, this added about <strong>0.2 ms</strong> of latency per frame, worth eliminating
                    especially that it's on the CPU's side.
                </p>

                <h3>Solution: Direct CUDA→OpenGL Texture Mapping</h3>
                <p>
                    By using CUDA ↔ OpenGL interop (via <code>cudaGraphicsGLRegisterImage</code> /
                    <code>cudaGraphicsMapResources</code>),
                    we can render directly into an OpenGL texture's memory, completely bypassing CPU staging:
                </p>
                <pre><code class="language-cpp">// During init
cudaGraphicsGLRegisterImage(&cudaTexRes, glTextureID, GL_TEXTURE_2D, cudaGraphicsRegisterFlagsWriteDiscard);

// Each frame
cudaGraphicsMapResources(1, &cudaTexRes);
cudaGraphicsSubResourceGetMappedArray(&mappedArray, cudaTexRes, 0, 0);

// Render to image from kernel here...

cudaGraphicsUnmapResources(1, &cudaTexRes);

// Then simply draw the GL texture on screen</code></pre>
            </section>

            <section class="section-header" id="honorable-mentions">
                <h2>Honorable Optimization Mentions</h2>

                <p>
                    These are optimizations I experimented with that <strong>did not yield measurable performance
                        gains</strong> in my specific use case. However, they are worth knowing and considering in
                    other contexts, where they may provide significant speedups.
                </p>

                <section class="section-header">

                    <h3>1. <code>__restrict__</code> Pointers</h3>
                    <p>
                        The <code>__restrict__</code> keyword is a compiler hint that tells the compiler: "<em>This
                            pointer is the only way to access the memory it points to for the duration of its
                            scope.</em>" This allows the compiler to make aggressive optimizations, such as reordering
                        memory accesses or avoiding redundant loads.
                    </p>

                    <pre><code class="language-cpp">__device__ void EvaluateShading(float* __restrict__ outLuminance,
                                const float* __restrict__ normal,
                                const float* __restrict__ albedo) {
    // Implementation that assumes no aliasing between input/output pointers
}</code></pre>

                    <p>
                        In my case, applying <code>__restrict__</code> to CUDA kernel arguments made no measurable
                        difference in performance, possibly because:
                    <ul>
                        <li>My kernels already had minimal pointer aliasing.</li>
                        <li>Access patterns were straightforward.</li>
                        <li>The compiler may have already inferred no aliasing from the context.</li>
                    </ul>
                    </p>

                    <div class="gotcha-card pro-tip">
                        <div class="gotcha-marker pro-tip-marker"></div>
                        <div class="gotcha-content">
                            <h3>Warning</h3>

                            <strong>⚠️ Use with caution:</strong> If you use <code>__restrict__</code> and the pointers
                            <em>do alias</em> — meaning they point to overlapping memory — the compiler's optimizations
                            will
                            lead to <strong>undefined behavior</strong> and likely subtle, incorrect results.

                        </div>
                    </div>


                    <p><strong>Example of safe usage (non-aliasing):</strong></p>
                    <pre><code class="language-cpp">__device__ void AddVectors(float* __restrict__ out,
                           const float* __restrict__ a,
                           const float* __restrict__ b) {
    out[0] = a[0] + b[0]; // Compiler assumes out, a, and b don't overlap
}</code></pre>

                    <p><strong>But what if they do alias?</strong></p>
                    <pre><code class="language-cpp">// Potentially undefined behavior if a and out alias
float data[3] = {1.0f, 2.0f, 3.0f};
AddVectors(data, data, &data[1]);
// 'a' and 'out' point to overlapping memory
</code></pre>

                    <p>
                        The compiler may optimize the load/store order assuming no aliasing (due to
                        <code>__restrict__</code>), and this can lead to incorrect results. For instance, it might cache
                        <code>a[0]</code> before realizing it was overwritten by another pointer.
                    </p>

                    <p>
                        This kind of subtle bug is particularly dangerous in GPU code where memory aliasing might occur
                        implicitly due to shared memory or register reuse, and debugging is difficult.
                    </p>

                    <p class="perf-note">
                        ✱ Verdict: <em>Potentially powerful, but dangerous if misused. Use only when you're
                            <strong>certain</strong> pointers do not alias.</em>
                    </p>
                </section>
                <section class="section-header">

                    <h3>2. <code>[[likely]]</code> and <code>[[unlikely]]</code> Attributes</h3>
                    <p>
                        These C++20 attributes allow you to hint to the compiler which branches are expected to be taken
                        more often. This can help the compiler generate more optimal branch prediction layouts, reducing
                        misprediction penalties on the CPU.
                    </p>

                    <p><strong>Example:</strong></p>
                    <pre><code class="language-cpp">// Process leaf node
if (node.Right == UINT32_MAX) [[unlikely]]
{
    // Process hit test
    hitAnything |= Hitables::IntersectPrimitive(ray, tmin, tmax, bestHit, node.Left);

    currentNode = stackData[--stackPtr];
    continue;
}
else [[likely]]
{
    // intersect child nodes...
}</code></pre>

                    <p>
                        In this example, you're telling the compiler that the first branch is the common case. This
                        might
                        improve performance slightly on CPUs by aligning the hot path better with the CPU's instruction
                        cache and branch predictor.
                    </p>

                    <p>
                        However, the actual benefit is usually small and heavily dependent on the target architecture
                        and
                        compiler. On GPU (e.g., CUDA), these attributes are ignored entirely by the compiler as of
                        today.
                    </p>

                    <p class="perf-note">
                        * Verdict: <em>Safe to use on CPU in hot paths with known branch probabilities. No effect in
                            CUDA.
                            Only use when you're confident about the branch behavior.</em>
                    </p>
                </section>

                <section class="section-header">
                    <h3>3. Targeting Very High SM Occupancy</h3>
                    <p>
                        It's common advice in CUDA optimization to aim for high Streaming Multiprocessor (SM) occupancy,
                        i.e., running many threads in parallel to hide memory latency. But in practice — especially for
                        workloads like path tracing — this isn't always beneficial.
                    </p>

                    <p>
                        Path tracing is inherently divergent due to the randomness of rays and material scattering
                        logic.
                        This limits how much benefit you get from higher occupancy. When threads in the same warp take
                        different execution paths, CUDA has to serialize their execution, which leads to reduced
                        efficiency.
                    </p>

                    <p>
                        To increase occupancy (more warps in flight), I experimented with larger block sizes and
                        different amounts of registers per thread:
                    </p>

                    <table class="perf-table glow-table">
                        <thead>
                            <tr>
                                <th>Block Size</th>
                                <th>Registers per Thread</th>
                                <th>Warps per Block</th>
                                <th>Theoretical Occupancy</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>4x8 (32)</td>
                                <td>64</td>
                                <td>1</td>
                                <td>33.3%</td>
                            </tr>
                            <tr>
                                <td>8x8 (64)</td>
                                <td>64</td>
                                <td>2</td>
                                <td>66.6%</td>
                            </tr>
                            <tr>
                                <td>8x12 (96)</td>
                                <td>&lt; 48</td>
                                <td>3</td>
                                <td>81%</td>
                            </tr>
                            <tr>
                                <td>8x16 (128)</td>
                                <td>&lt; 39</td>
                                <td>4 (Full SM)</td>
                                <td>100%</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Conclusion</h4>
                    <p>
                        While higher theoretical occupancy—such as that achieved with 128-thread blocks—can help hide
                        latency and improve throughput,
                        the observed performance differences across various block sizes remained marginal.
                        In this particular case, profiling revealed an average of <strong>only 20 active threads per
                            warp</strong>,
                        indicating significant <em>warp divergence</em>.
                    </p>
                    <p>
                        This divergence reduces the effective parallelism, meaning the GPU can't fully utilize its
                        execution units,
                        even when plenty of warps are available. As a result, simply increasing occupancy doesn't
                        translate into better performance because:
                    </p>
                    <ul>
                        <li><strong>Warp divergence</strong> becomes a bottleneck, not occupancy.</li>
                        <li><strong>Instruction throughput and memory latency</strong> are only hidden if active warps
                            are available—and in this case, many warps are partially idle.</li>
                        <li>Optimizations like reducing register usage or maximizing block size become less impactful
                            when execution efficiency per warp is already low.</li>
                    </ul>
                    <p>
                        In short, the key insight is that <strong>occupancy alone isn’t enough</strong>—<strong>warp
                            efficiency matters more</strong>.
                        To unlock better performance, you’d likely gain more by restructuring code to <em>minimize
                            divergence</em>
                        (e.g., avoiding divergent branches inside warps) than by merely tuning block sizes or register
                        counts.
                    </p>

                    <img src="images/RTIOW/Screenshot 2025-06-12 104519.png">
                    <p>
                        See the low <strong>Issued Warp Per Scheduler</strong>? This means that even though there are
                        warps ready and active,
                        very few instructions are being issued each cycle—indicating poor utilization of the scheduler’s
                        throughput.
                    </p>
                    <p>
                        One likely reason is that a traditional megakernel path tracer often contains complex, deeply
                        branched logic—such as handling BVH traversal, Russian Roulette, and more—all inside a single
                        big kernel.
                        This causes significant
                        <em>warp divergence</em> as different threads take different paths through the code.
                    </p>
                    <p>
                        <strong>Wavefront Path Tracing</strong> can help here by decomposing the work into separate,
                        simpler stages (e.g., ray generation,
                        intersection, shading). Each stage is executed by a specialized kernel operating
                        on batches of work units that are more uniform.
                        This means threads within a warp are more likely to follow the same execution path, dramatically
                        improving warp efficiency and instruction issue rate.
                    </p>
                    <p>
                        By separating divergent logic into more uniform waves, wavefront architecture increases the
                        likelihood that <strong>multiple instructions can be issued per cycle</strong>,
                        improving the <em>Issued Warp Per Scheduler</em> metric and ultimately leading to higher overall
                        throughput.
                    </p>


                    <p class="perf-note">
                        ✱ Verdict: <em>Don't chase maximum occupancy blindly. In divergent workloads like path tracing,
                            higher register count and fewer threads may outperform higher occupancy configurations.</em>
                    </p>
                </section>


                <section class="section-header">
                    <h3>4. Van Emde Boas Layout for BVH (Post-Build Sort)</h3>
                    <p>
                        One lesser-known but promising optimization is applying a <strong>Van Emde Boas (VEB)
                            layout</strong> to the Bounding Volume Hierarchy (BVH) as a post-processing step. The goal
                        is
                        to improve spatial locality, increasing the chance that memory accesses during traversal stay in
                        the
                        L1 cache.
                    </p>

                    <p>
                        In this path tracer, I'm using a Struct of Arrays (SoA) layout, which is already cache-friendly.
                        That means each property (e.g., positions, radii, materials) is tightly packed and accessed
                        sequentially — ideal for coalesced GPU reads.
                    </p>

                    <p><strong>Scene Stats:</strong></p>
                    <ul>
                        <li>488 spheres (center and radius) x sizeof(Vec4) = <strong>7'808 bytes</strong></li>
                        <li>488 materials x 2 x sizeof(Vec4) = <strong>15'616 bytes</strong></li>
                        <li>975 BVH nodes x sizeof(AABB) = <strong>31'200 bytes</strong></li>
                        <li>975 BVH nodes x sizeof(BVH node metadata) = <strong>7'800 bytes</strong></li>
                        <li><strong>Total: 62'424 bytes</strong></li>
                    </ul>

                    <p>
                        Since this is well within a 256KB L1 cache, applying a VEB layout to the BVH would likely bring
                        no
                        meaningful performance gain in this particular case. However, in larger scenes — or if the BVH
                        and
                        primitives span multiple cache lines — this optimization can help reduce cache misses during
                        traversal.
                    </p>

                    <p><strong>What VEB Does:</strong></p>
                    <ul>
                        <li>Reorders the BVH in memory to follow a cache-oblivious, recursive layout.</li>
                        <li>Ensures nodes accessed close together during traversal are stored close together in memory.
                        </li>
                        <li>Reduces memory latency and improves prefetch effectiveness on wide cache lines.</li>
                    </ul>

                    <p class="perf-note">
                        ✱ Verdict: <em>Not needed for small scenes fitting comfortably in L1 cache. But <strong>can be a
                                big
                                win</strong> for large, cache-stretching BVHs in real-world rendering workloads.</em>
                    </p>
                </section>

                <section class="section-header">
                    <h3 class="honorable-mention-title">5. Parallelizing SAH BVH Construction
                    </h3>

                    <p>
                        Building a Surface Area Heuristic (SAH) BVH is a key step in path tracing acceleration.
                        While
                        SAH
                        construction is often viewed as expensive, it's highly parallelizable on GPUs. However, this
                        optimization requires careful implementation and wasn't something I tackled yet.
                    </p>

                    <p>
                        Currently, my BVH construction is implemented recursively, which is efficient on the CPU but
                        <strong>especially slow on GPUs</strong> due to recursion. Even stack size has to be set
                        manually.
                    </p>

                    <p>
                        Interestingly, in my simple tests (single-threaded CPU vs single-threaded GPU), the CPU
                        version
                        was
                        <strong>dramatically faster</strong>:
                    </p>

                    <table class="perf-table">
                        <thead>
                            <tr>
                                <th>Implementation</th>
                                <th>BVH Construction Time</th>
                                <th>Threads</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>GPU (single-threaded, recursive)</td>
                                <td>58.694 ms</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>CPU (single-threaded, recursive)</td>
                                <td>0.829 ms</td>
                                <td>1</td>
                            </tr>
                        </tbody>
                    </table>

                    <p>
                        This result shocked me — CPUs have advanced scalar cores with sophisticated branch
                        prediction
                        and
                        fast caches that make single-threaded SAH extremely efficient. Meanwhile, GPU SAH builds
                        without parallelization and using <em>recursion</em> struggle significantly.
                    </p>

                    <p>
                        <strong>Takeaway:</strong> SAH BVH construction <em>can</em> be massively accelerated on the
                        GPU
                        with well-designed parallel and iterative algorithms. It's a worthy optimization for future
                        work
                        but
                        requires rewriting recursive parts and handling GPU-specific challenges like thread
                        divergence
                        and
                        memory management.
                    </p>

                    <p class="perf-note">
                        ✱ Verdict: <em>Great potential but complex. Worth exploring if BVH build times become a
                            bottleneck.</em>
                    </p>


                </section>
            </section>

            <section class="section-header">
                <h2 class="section-title">Future Work</h2>

                <p>
                    As with any graphics project, there's always something cooler, faster, or more unnecessarily
                    complicated
                    to try. Here's what's on my radar for future exploration:
                </p>

                <h3 class="subsection-title">🌊 Wavefront Path Tracing</h3>
                <p>
                    Traditional <em>megakernels</em>—like the one I'm currently using—tend to suffer from high register
                    pressure, warp divergence, and other GPU ailments. Wavefront path tracing offers a remedy by
                    breaking
                    the pipeline into separate, parallel stages (generate rays, intersect, shade, etc.), which allows
                    better
                    control over occupancy and divergence.
                </p>

                <p>
                    There's compelling evidence that wavefront is significantly faster under the right conditions:
                    <a href="https://www.reddit.com/r/GraphicsProgramming/comments/1gnfokb/why_is_wavefront_path_tracing_5x_times_faster/"
                        target="_blank">Reddit
                        discussion on wavefront being 5x faster</a><br>
                    And a solid academic backbone from NVIDIA Research:
                    <a href="https://research.nvidia.com/sites/default/files/pubs/2013-07_Megakernels-Considered-Harmful/laine2013hpg_paper.pdf"
                        target="_blank">Megakernels
                        Considered Harmful — Laine & Karras</a>
                </p>

                <p>
                    It's not trivial to implement—requires work queues, persistent kernels, and possibly a mild
                    existential
                    crisis—but it's one of the best-known performance unlocks for complex scenes.
                </p>

                <h3 class="subsection-title">📐 Triangle Support via TinyBVH?</h3>
                <p>
                    While my current path tracer only supports spheres (like it's still 2004), the next logical step is
                    supporting triangle geometry. I'm exploring <a href="https://github.com/jbikker/tinybvh"
                        target="_blank">tinybvh</a>
                    for fast triangle BVH construction and traversal.
                </p>

                <p>
                    This will open the door to loading full mesh scenes (like Cornell Boxes and Sponza), potentially
                    with
                    hardware-accelerated triangle intersection via CUDA intrinsics.
                </p>

                <h3>💥 OptiX Backend</h3>
                <p>
                    Eventually, I'd like to compare this CUDA implementation with an <strong>OptiX-based path
                        tracer</strong>. That would allow testing hardware ray tracing (RT cores) vs. our current
                    software traversal, and take advantage of the mature BVH traversal stack in OptiX.
                </p>

                <h3 class="subsection-title">💼 Also... I'm Looking for a Job 😅</h3>
                <p>
                    If you've made it this far, first of all: respect. Second of all, hey—I'm actively looking for a
                    role in
                    graphics programming, CUDA/GPU engineering, or real-time rendering. So if you're a hiring
                    manager, <strong>let's
                        talk</strong>. Check my resume, or just shoot me a message. I'll even throw in a free bug fix.
                </p>
            </section>

            <section class="section-header">
                <h2 class="section-title">References</h2>
                <ul class="reference-list">
                    <li>
                        <a href="https://raytracing.github.io/" target="_blank"><em>Ray Tracing in One Weekend</em>
                            by Peter Shirley</a>
                        <br>
                        A classic introductory book series that helped shape the structure of my path tracer. Available
                        freely at:
                    </li>
                    <li>
                        <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html"
                            target="_blank">NVIDIA CUDA Programming Guide</a>
                        <br>
                        The definitive guide for CUDA architecture, performance tips, and memory behavior. Especially
                        useful
                        for understanding occupancy, warp scheduling, and function inlining.
                    </li>
                    <li>
                        <a href="https://developer.nvidia.com/blog/" target="_blank">Optimizing CUDA by Nvidia
                            DevBlog</a>
                        <br>
                        Multiple performance blog posts from NVIDIA covering best practices, launch configuration,
                        memory
                        coalescing.</code>.
                    </li>

                    <li>
                        <a href="https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/"
                            target="_blank">Accelerated Ray
                            Tracing in CUDA — NVIDIA Developer Blog</a>
                        <br>
                        A practical overview on implementing ray tracing using CUDA, including memory layout, BVH
                        traversal,
                        and shading strategies.
                    </li>
                    <li>
                        <a href="https://github.com/GPSnoopy/RayTracingInVulkan" target="_blank">GPSnoopy's
                            RayTracingInVulkan</a>
                        <br>
                        A well-structured real-time ray tracing renderer in Vulkan with RTX support.
                    </li>
                    <li>
                        <a href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf"
                            target="_blank">
                            NVIDIA Ampere GA102 GPU Architecture Whitepaper (PDF)
                        </a>
                        <br>
                        A valuable source when it comes to architecture-specific optimizations. It says FP16(non-Tensor)
                        runs at full speed as FP32 on the Ampere architecture.
                    </li>

                </ul>
            </section>


            <section class="section-header">
                <h2>Disscussions</h2>

                <div id="disqus_thread"></div>
                <script>
                    /**
                    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
                    var disqus_config = function () {
                        this.page.url = window.location.href;
                        this.page.identifier = window.location.pathname;
                    };

                    (function () { // DON'T EDIT BELOW THIS LINE
                        var d = document, s = d.createElement('script');
                        s.src = 'https://https-karimsayedre-github-io.disqus.com/embed.js';
                        s.setAttribute('data-timestamp', +new Date());
                        (d.head || d.body).appendChild(s);
                    })();
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments
                        powered
                        by
                        Disqus.</a></noscript>
            </section>
    </div>


    <script>
        footer();
        addBehaviour();
    </script>

    <div style="text-align: center;">
        <div>Visitor Count: <span id="visits">Loading...</span></div>
        <script>
            fetch('https://abacus.jasoncameron.dev/hit/karimsayedre.github.io/RTIOW.html')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('visits').innerText = data.value;
                });
        </script>
    </div>

    <!-- Navigation buttons -->
    <div class="nav-buttons">

        <div class="scroll-button scroll-up" onclick="scrollToTop()" title="Go to top"><img src="icons/arrow.png">
        </div>
        <div class="scroll-button scroll-down" onclick="scrollToBottom()" title="Go to bottom"><img
                src="icons/arrow.png"></div>
    </div>

    <div id="bottom"></div>

    <script
        src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.8.0/dist/highlightjs-line-numbers.min.js"></script>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", () => {
            document.querySelectorAll('pre').forEach((block) => {
                const button = document.createElement('button');
                button.innerText = 'Copy';
                button.className = 'copy-button';
                block.appendChild(button);
            });

            const clipboard = new ClipboardJS('.copy-button', {
                target: (trigger) => trigger.previousElementSibling,
            });

            clipboard.on('success', (e) => {
                e.trigger.innerText = 'Copied!';
                setTimeout(() => (e.trigger.innerText = 'Copy'), 1500);
            });
        });
    </script>

    <script>
        function scrollToTop() {
            document.getElementById('top').scrollIntoView({
                behavior: 'smooth'
            });
        }

        function scrollToBottom() {
            document.getElementById('bottom').scrollIntoView({
                behavior: 'smooth'
            });
        }
    </script>

    <script>
        document.addEventListener("DOMContentLoaded", () => {
            hljs.highlightAll();
            hljs.initLineNumbersOnLoad();
        });
    </script>
    <!-- <script id="dsq-count-scr" src="//https-karimsayedre-github-io.disqus.com/count.js" async></script> -->
</body>

</html>